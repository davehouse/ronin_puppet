<system>
  root_dir /var/log/td-agent         # For handling unrecoverable chunks
</system>

<source>
  tag system
  @type exec
  # log show entries since boot
  command "\
    log show --last boot --info --color none --style default \
      --predicate 'eventType == \"logEvent\" && (process MATCHES \"(logger|sudo|sshd)\" || messageType >= <%= @mac_log_level %>)' \
    | sed -l -e '1d' \
    "
  <parse>
    @type regexp
    expression /^(?<time>.{31})\s+(?<thread>\S+)\s+(?<severity>\S+)\s+(?<activity>\S+)\s+(?<pid>\S+)\s+(?<ttl>\S+)\s+((?<program>\S+):\s+)?(?<log_message>.*)$/
    types pid:integer
    time_key time
    time_format "%Y-%m-%d %H:%M:%S.%N%z"
  </parse>
</source>

<source>
  tag system
  @type exec
  # log stream from now
  command "\
    log stream --type log --level info --color none \
      --predicate '(process MATCHES \"(logger|sudo|sshd)\" || messageType >= <%= @mac_log_level %>)' \
      --timeout 1d \
    | sed -l -e '2d' \
    "
    # messageType integers:
    #   0 debug
    #   1 info
    #   2 warn
    #   16 error
    #   17 crit
    # tail to omit first 2 lines that cause parsing to fail:
    #   Filtering the log data using "type == 1024"
    #   Timestamp                       Thread     Type        Activity             PID    TTL
    #   2019-05-17 22:49:59.070398+0000 0x195      Info        0x0                  0      0    kernel: (IOHIDFamily) HID Activity Tickle (type:0 sender:100007033)
    # tail with bsd sed `-l` (line-buffers instead of stdbuf)
  <parse>
    @type regexp
    # getting tsv or csv requires reformatting in the command pipeline
    # log can output json, but
    # 1. it is a streamed array (req. cuts+matches in pipe)
    # 2. json plugin in mac fluentd has bugs dropping entries and buffering
    expression /^(?<time>.{31})\s+(?<thread>\S+)\s+(?<severity>\S+)\s+(?<activity>\S+)\s+(?<pid>\S+)\s+(?<ttl>\S+)\s+((?<program>\S+):\s+)?(?<log_message>.*)$/
    types pid:integer
    time_key time
    time_format "%Y-%m-%d %H:%M:%S.%N%z"
  </parse>
  # callback will error or warn if it stopped
  # Do not use run_interval:
  # 1. run_interval delays first execution to end of first interval
  # 2. run_interval buffers output
</source>

<filter system>
  @type record_transformer
  enable_ruby true
  <record>
    # MacOS Unified logging levels: Fault, Error, Default, Info, Debug
    # error="'Default' is not a designated severity"
    # syslog severities:
    #   https://github.com/eric/syslog_protocol/blob/master/lib/syslog_protocol/common.rb#L58
<% if @syslog_host =~ /papertrail/ -%>
    severity #{(record["severity"]||"Error").gsub(/^(Info|Default|Error|Fault)/i, 'Info' => 'info', 'Default' => 'warn', 'Error' => 'err', 'Fault' => 'crit')}
<% elsif @syslog_host != '' -%>
    syslog_severity #{(record["severity"]||"Error").gsub(/^(Info|Default|Error|Fault)/i, 'Info' => 'info', 'Default' => 'warn', 'Error' => 'err', 'Fault' => 'crit')}
<% end -%>
<% if @stackdriver_clientid == '' -%>
    message \[${record["pid"]}\]: ${record["log_message"]}
<% else -%>
    message ${record["program"]}\[${record["pid"]}\]: ${record["log_message"]}
    # stackdriver severities:
    #   https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#LogSeverity
    stackdriver_severity #{(record["severity"]||"Error").gsub(/^(Info|Default|Error|Fault)/i, 'Info' => 'info', 'Default' => 'warning', 'Error' => 'error', 'Fault' => 'critical')}
<% end -%>
  </record>
</filter>


<source>
  tag syslog
  @type tail
  read_from_head true
  path /var/log/system.log
  pos_file /var/log/system.log.pos
  <parse>
    # standard syslog parsing often fails because the macos system.log has inconsistent multi-line, space delimiters, and ident/pid formatting
    # @type syslog
    # with_priority false
    # parser_type string
    @type multiline
    format_firstline /^[A-Za-z]{3} +\d+ \d{2}:\d{2}:\d{2} /
    # ugly regex that handles the varieties of macos syslog lines I've tested
    format1 /^(?<time>[A-Za-z]{3} +\d+ \d{2}:\d{2}:\d{2}) [^ ]+ (?<ident>[^\[ :]*)(?<message>[\[\]A-Za-z: ]*(\[(?<pid>\d+)\]:?)? .*)/
    time_key time
    time_format "%b %d %H:%M:%S"
    keep_time_key true
  </parse>
</source>
<filter syslog>
  @type record_transformer
  <record>
    severity info
    program ${record["ident"]}
    facility local0
  </record>
</filter>
<filter syslog>
  @type grep
  <regexp>
    key program
    pattern /^sshd$/
  </regexp>
</filter>


<% if @tail_worker_logs -%>
<source>
  tag worker.err
  @type tail
  read_from_head true
  path <%= @worker_stderr %>
  pos_file <%= @worker_stderr %>.pos
  <parse>
    @type multiline
    # 2019/11/07 18:41:43 Disk available: 225110376448 bytes\n
    format_firstline /^\d{4}\/\d{2}\/\d{2} \d{2}:\d{2}:\d{2}Z? /
    format1 /^(?<log_time>\d{4}\/\d{2}\/\d{2} \d{2}:\d{2}:\d{2})Z?( [A-Z]{3})? +(?<log_message>.*)/
    time_key log_time
    time_format "%Y/%m/%d %H:%M:%S"
    keep_time_key true
  </parse>
</source>
<source>
  tag worker.info
  @type tail
  read_from_head true
  path <%= @worker_stdout %>
  pos_file <%= @worker_stdout %>.pos
  <parse>
    @type multiline
    # 2019/11/07 18:41:43 Disk available: 225110376448 bytes\n
    format_firstline /^\d{4}\/\d{2}\/\d{2} \d{2}:\d{2}:\d{2}Z? /
    format1 /^(?<log_time>\d{4}\/\d{2}\/\d{2} \d{2}:\d{2}:\d{2})Z?( [A-Z]{3})? +(?<log_message>.*)/
    time_key log_time
    time_format "%Y/%m/%d %H:%M:%S"
    keep_time_key true
  </parse>
</source>

<filter worker.**>
  @type grep
  <exclude>
    key log_message
    pattern /^Disk available: \d+ bytes$/
  </exclude>
</filter>

<filter worker.**>
  @type record_transformer
  enable_ruby true
  <record>
    program "worker"
    # single user:
    # No task claimed. Idle for 5h15m43.362805902s (will exit if no task claimed in 90h44m16.637194098s). 1 more tasks to run before exiting.
    # No task 5h15m43 90h44m16 1
    # multi-user:
    # No task claimed. Idle for 45h41m6.936831744s (will exit if no task claimed in 50h18m53.063168256s).
    # No task 45h41m6 50h18m53
<% if @syslog_host =~ /papertrail/ -%>
    severity "${tag_parts[1]}"
<% elsif @syslog_host != '' -%>
    syslog_severity "${tag_parts[1]}"
<% end -%>
<% if @stackdriver_clientid != '' -%>
    # stackdriver severities:
    #   https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#LogSeverity
    stackdriver_severity "${tag_parts[1].gsub(/^(err|info)/i, 'info' => 'info', 'err' => 'error')}"
    #message #{record["program"]} #{record["log_message"].sub(/^(No task) claimed. Idle for ([^\.]+)\..* (\d+) more tasks to run before exiting\.$/, '\1 \2 \3')}
    message #{record["program"]} #{record["log_message"].sub(/^(No task) claimed. Idle for ([^\.]+)\..*$/, '\1 \2')}
<% else -%>
    message #{record["log_message"].sub(/^(No task) claimed. Idle for ([^\.]+)\..*$/, '\1 \2')}
<% end -%>
    # No task claimed. Idle for 5h15m43.362805902s (will exit if no task claimed in 90h44m16.637194098s). 1 more tasks to run before exiting.
    # No task 5h15m43 1
  </record>
</filter>
<% end -%>

<filter fluent.error>
  @type record_transformer
  enable_ruby true
  <record>
    severity "err"
  </record>
</filter>
<match fluent.{trace,debug,info,warn}>
  @type null
</match>

<% if @stackdriver_clientid != '' -%>
# Add a unique insertId to each log entry that doesn't already have it.
# This helps guarantee the order and prevent log duplication.
<filter **>
  @type add_insert_ids
</filter>

# string/numbers only
# msgpack cannot encode ruby Time fields
<filter **>
  @type record_transformer
  remove_keys ["host"]
  <record>
    hostname "#{Socket.gethostname}"
  </record>
  <record>
    workerId "#{Socket.gethostname.split('.')[0]}"
  </record>
  <record>
    workerType "<%= @worker_type %>"
  </record>
  <record>
    workerGroup "#{Socket.gethostname.split('.')[3]}"
  </record>
  <record>
    severity #{record["stackdriver_severity"]}
  </record>
</filter>
<% end -%>


<match **>
  @type copy

<% if @syslog_host =~ /papertrail/ -%>
  <store>
    @type papertrail
    papertrail_host <%= @syslog_host %>
    papertrail_port <%= @syslog_port %>
    default_hostname "#{Socket.gethostname.split('.')[0]}"
    <buffer>
      @type file
      path /var/log/td-agent/papertrail_buffer
      flush_mode interval
      retry_type exponential_backoff
      retry_max_interval 97s
      retry_forever true
      flush_interval 1s
      flush_at_shutdown true
    </buffer>
  </store>
  # debug:
  # <store>
  #   @type "file"
  #   path "/var/log/td-agent/debuglog"
  #   <inject>
  #     time_format "%Y%m%dT%H%M%S%z"
  #     time_key "time"
  #     localtime false
  #   </inject>
  #   <buffer time>
  #     path "/var/log/td-agent/debuglog"
  #   </buffer>
  # </store>

<% elsif @syslog_host != '' -%>
  <store>
    @type remote_syslog
    hostname "#{Socket.gethostname.split('.')[0]}"
    host <%= @syslog_host %>
    port <%= @syslog_port %>
    <buffer program,syslog_severity>
      flush_at_shutdown true
      overflow_action block
    </buffer>
    program ${program}
    severity ${syslog_severity}
    <format>
      @type single_value
      message_key message
    </format>
  </store>
<% else -%>
# Disabled syslog output. (syslog_host: <%= @syslog_host %>)
<% end -%>

<% if @stackdriver_clientid != '' -%>
  <store>
    @type google_cloud
    use_metadata_service false
    vm_id "#{Socket.gethostname}"
    zone "#{Socket.gethostname.split('.')[3]}"
    # Set the chunk limit conservatively to avoid exceeding the recommended
    # chunk size of 5MB per write request.
    buffer_chunk_limit 1M
    # Flush logs every 5 seconds, even if the buffer is not full.
    flush_interval 5s
    # Enforce some limit on the number of retries.
    disable_retry_limit false
    # After 3 retries, a given chunk will be discarded.
    retry_limit 3
    # Wait 10 seconds before the first retry. The wait interval will be doubled on
    # each following retry (20s, 40s...) until it hits the retry limit.
    retry_wait 10
    # Never wait longer than 5 minutes between retries. If the wait interval
    # reaches this limit, the exponentiation stops.
    # Given the default config, this limit should never be reached, but if
    # retry_limit and retry_wait are customized, this limit might take effect.
    max_retry_wait 300
    # Use multiple threads for processing.
    num_threads 8
    detect_json true
    # Use the gRPC transport.
    use_grpc true
    # If a request is a mix of valid log entries and invalid ones, ingest the
    # valid ones and drop the invalid ones instead of dropping everything.
    partial_success true
  </store>
<% else -%>
# Disabled stackdriver output. (clientid: <%= @stackdriver_clientid %>)
<% end -%>
</match>
